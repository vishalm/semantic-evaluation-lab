# ===============================================================================
# Semantic Evaluation Lab - Environment Configuration
# ===============================================================================
# This file contains all environment variables for automated lab operation.
# Copy this file to .env and customize the values for your environment.

# ===============================================================================
# CORE AI CONFIGURATION
# ===============================================================================
# Primary AI service configuration
USE_OLLAMA=true
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL_ID=qwen2.5:latest

# Azure OpenAI Configuration (if USE_OLLAMA=false)
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-35-turbo
AZURE_OPENAI_API_VERSION=2024-02-01

# OpenAI API Key (required for DeepEval metrics)
OPENAI_API_KEY=your-openai-api-key-here

# ===============================================================================
# LAB ENVIRONMENT CONFIGURATION
# ===============================================================================
# Basic lab identification and settings
LAB_ENVIRONMENT=development
LAB_NAME=Semantic-Evaluation-Lab
LAB_VERSION=1.0.0
LAB_ADMIN_EMAIL=admin@lab.local
ENABLE_DEBUG_MODE=true

# Agent Configuration
AGENT_NAME=SEL-Assistant
AGENT_INSTRUCTIONS=You are a helpful assistant in the Semantic Evaluation Lab.

# ===============================================================================
# AUTO-TRIGGER CONFIGURATION
# ===============================================================================
# Master switch for all automation
AUTO_RUN_TESTS=false

# Individual test suite automation
AUTO_RUN_UNIT_TESTS=true
AUTO_RUN_FUNCTIONAL_TESTS=false
AUTO_RUN_LLM_EVAL_TESTS=false
AUTO_RUN_CONVERSATION_TESTS=false
AUTO_RUN_LOAD_TESTS=false

# Setup and reporting automation
AUTO_SETUP_MODELS=true
AUTO_GENERATE_REPORTS=true

# ===============================================================================
# TEST CONFIGURATION
# ===============================================================================
# Test execution settings
TEST_TIMEOUT_SECONDS=1800
TEST_RETRY_COUNT=3
TEST_PARALLEL_WORKERS=2

# Conversation chain testing
CONVERSATION_CHAIN_LENGTHS=5,10,15
ENABLE_CONVERSATION_STABILITY_TESTS=true
ENABLE_DYNAMIC_CONVERSATION_TESTS=true

# ===============================================================================
# LOAD TESTING CONFIGURATION
# ===============================================================================
# Basic load test parameters
LOCUST_USERS=1
LOCUST_SPAWN_RATE=1
LOCUST_RUN_TIME=300s

# Load test targets and quality
LOAD_TEST_TARGET_HOST=http://localhost:8000
ENABLE_LOAD_TEST_AUTO_SCALING=false
LOAD_TEST_QUALITY_THRESHOLD=0.7

# ===============================================================================
# MONITORING & OBSERVABILITY
# ===============================================================================
# Core monitoring settings
ENABLE_MONITORING=true
PROMETHEUS_RETENTION_TIME=7d
PROMETHEUS_RETENTION_SIZE=1GB

# Service ports
PROMETHEUS_PORT=8000
GRAFANA_PORT=3000
NODE_EXPORTER_PORT=9100

# Grafana configuration
GRAFANA_ADMIN_PASSWORD=admin

# Monitoring features
ENABLE_ALERTING=true
METRICS_COLLECTION_INTERVAL=15s
ENABLE_OLLAMA_METRICS=true

# ===============================================================================
# DEEPEVAL CONFIGURATION
# ===============================================================================
# DeepEval framework settings
DEEPEVAL_MODEL_ID=gpt-4
DEEPEVAL_CACHE_FOLDER=.deepeval_cache
DEEPEVAL_VERBOSE=true
DEEPEVAL_CONFIDENCE_AI_TOKEN=your-confident-ai-token-here

# Quality thresholds for evaluation
ANSWER_RELEVANCY_THRESHOLD=0.7
FAITHFULNESS_THRESHOLD=0.7
RESPONSE_QUALITY_THRESHOLD=0.75

# ===============================================================================
# LOGGING CONFIGURATION
# ===============================================================================
# Logging behavior and format
LOG_LEVEL=INFO
ENABLE_STRUCTURED_LOGGING=true
ENABLE_JSON_LOGGING=true
LOG_AGGREGATION_ENABLED=true
LOG_RETENTION_DAYS=30

# ===============================================================================
# NOTIFICATION CONFIGURATION
# ===============================================================================
# Alert and notification settings
ENABLE_NOTIFICATIONS=false

# Slack Integration
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK

# Discord Integration
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/YOUR/DISCORD/WEBHOOK

# Email Configuration
EMAIL_SMTP_HOST=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_USERNAME=your-email@example.com
EMAIL_PASSWORD=your-app-password

# ===============================================================================
# CI/CD INTEGRATION
# ===============================================================================
# Continuous integration settings
CI_MODE=false
GITHUB_TOKEN=your-github-token-here

# Code quality integration
ENABLE_SONAR_INTEGRATION=false
SONAR_TOKEN=your-sonar-token-here

# Build information
BUILD_NUMBER=local
GIT_COMMIT_SHA=unknown

# ===============================================================================
# PERFORMANCE TUNING
# ===============================================================================
# Resource limits and optimization
MAX_MEMORY_USAGE=1G
MAX_CPU_CORES=2
ENABLE_PERFORMANCE_PROFILING=false
ENABLE_MEMORY_MONITORING=true
CONTAINER_TIMEZONE=UTC

# ===============================================================================
# OLLAMA ADVANCED CONFIGURATION
# ===============================================================================
# Advanced Ollama settings
OLLAMA_MODELS=qwen2.5:latest,llama2:latest
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_KEEP_ALIVE=5m

# ===============================================================================
# EXAMPLE CONFIGURATIONS
# ===============================================================================

# Example 1: Quick Local Testing
# AUTO_RUN_TESTS=true
# AUTO_RUN_UNIT_TESTS=true
# AUTO_RUN_FUNCTIONAL_TESTS=true
# LOCUST_USERS=1
# LOCUST_RUN_TIME=120s

# Example 2: Full Evaluation Lab
# AUTO_RUN_TESTS=true
# AUTO_RUN_LLM_EVAL_TESTS=true
# AUTO_RUN_CONVERSATION_TESTS=true
# ENABLE_MONITORING=true
# ENABLE_NOTIFICATIONS=true

# Example 3: Load Testing Focus
# AUTO_RUN_LOAD_TESTS=true
# LOCUST_USERS=5
# LOCUST_SPAWN_RATE=1
# LOCUST_RUN_TIME=600s
# LOAD_TEST_QUALITY_THRESHOLD=0.8

# Example 4: Production Monitoring
# LAB_ENVIRONMENT=production
# ENABLE_DEBUG_MODE=false
# LOG_LEVEL=WARNING
# ENABLE_MONITORING=true
# ENABLE_ALERTING=true
# PROMETHEUS_RETENTION_TIME=30d

# ===============================================================================
# QUICK START CONFIGURATIONS
# ===============================================================================

# Minimal Setup (Default):
# - Ollama with qwen2.5:latest
# - Unit tests enabled
# - Basic monitoring
# - No external API keys needed

# Full Feature Setup:
# 1. Set OPENAI_API_KEY for LLM evaluation
# 2. Enable AUTO_RUN_LLM_EVAL_TESTS=true
# 3. Enable AUTO_RUN_CONVERSATION_TESTS=true
# 4. Set up notifications if desired

# Load Testing Setup:
# 1. Set AUTO_RUN_LOAD_TESTS=true
# 2. Configure LOCUST_* parameters
# 3. Set LOAD_TEST_QUALITY_THRESHOLD
# 4. Enable ENABLE_MONITORING=true for real-time metrics 